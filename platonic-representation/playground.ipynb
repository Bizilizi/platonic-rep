{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a97edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p DEADLINE\n",
      "#SBATCH -A test\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=24\n",
      "#SBATCH --mem=45G\n",
      "#SBATCH -t 01:00:00\n",
      "\n",
      "/home/wiss/zverev/miniconda3/envs/prh/bin/python -m distributed.cli.dask_worker tcp://131.159.19.243:36625 --name dummy-name --nthreads 1 --memory-limit 1.86GiB --nworkers 24 --nanny --death-timeout 60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/dask_jobqueue/slurm.py:55: FutureWarning: project has been renamed to account as this kwarg was used wit -A option. You are still using it (please also check config files). If you did not set account yet, project will be respected for now, but it will be removed in a future release. If you already set account, project is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 17:57:03,465 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7e0babed6a70>>, <Task finished name='Task-24' coro=<SpecCluster._correct_state_internal() done, defined at /home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/distributed/deploy/spec.py:352> exception=RuntimeError(\"Command exited with non-zero exit code.\\nExit code: 1\\nCommand:\\nsbatch /tmp/tmpw1z2i21v.sh\\nstdout:\\n\\nstderr:\\nsbatch: error: For deadline jobs, you must include a comment using the --comment parameter to explain the need for a deadline partition job\\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\\n\\n\")>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/tornado/ioloop.py\", line 782, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/distributed/deploy/spec.py\", line 396, in _correct_state_internal\n",
      "    await asyncio.gather(*worker_futs)\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/asyncio/tasks.py\", line 650, in _wrap_awaitable\n",
      "    return (yield from awaitable.__await__())\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "  File \"/home/wiss/zverev/miniconda3/envs/prh/lib/python3.10/site-packages/dask_jobqueue/core.py\", line 514, in _call\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Command exited with non-zero exit code.\n",
      "Exit code: 1\n",
      "Command:\n",
      "sbatch /tmp/tmpw1z2i21v.sh\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "sbatch: error: For deadline jobs, you must include a comment using the --comment parameter to explain the need for a deadline partition job\n",
      "sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dask_jobqueue\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "cluster = dask_jobqueue.SLURMCluster(\n",
    "    queue=\"DEADLINE\",\n",
    "    project=\"test\",\n",
    "    cores=24,\n",
    "    processes=24,\n",
    "    memory='48 GB',\n",
    "    walltime='01:00:00',\n",
    ")\n",
    "print(cluster.job_script())\n",
    "cluster.scale(jobs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
